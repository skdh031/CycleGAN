{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division \n",
    "import scipy \n",
    "\n",
    " \n",
    "from keras.datasets import mnist \n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization \n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate \n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D \n",
    "from keras.layers.advanced_activations import LeakyReLU \n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D \n",
    "from keras.models import Sequential, Model \n",
    "from keras.optimizers import Adam \n",
    "import datetime \n",
    "import matplotlib.pyplot as plt\n",
    "from utils.loaders import DataLoader\n",
    "import sys \n",
    "import numpy as np \n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN():\n",
    "    def __init__(self):\n",
    "        # 입력 이미지 shape\n",
    "        self.img_rows = 128 # 이미지의 가로 픽셀\n",
    "        self.img_cols = 128 # 이미지의 세로 픽셀\n",
    "        self.channels = 3   # 이미지 채널(색)\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # data loader 설정\n",
    "        self.dataset_name = 'apple2orange' # 데이터셋 디렉토리 이름\n",
    "        self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
    "                                      img_res=(self.img_rows, self.img_cols))\n",
    "\n",
    "\n",
    "        # Discriminator output 사이즈 계산 (PatchGAN 사용)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Generator 와 Discriminator 의 첫 번째 레이어 필터 개수\n",
    "        self.gf = 32\n",
    "        self.df = 64\n",
    "\n",
    "        # Loss weights\n",
    "        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n",
    "        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Discriminators 를 Build, compile\n",
    "        \n",
    "        self.d_A.build_discriminator()        \n",
    "        self.d_B.build_discriminator()\n",
    "        self.d_A.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        self.d_B.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generators\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generators\n",
    "        self.g_AB = self.build_generator()\n",
    "        self.g_BA = self.build_generator()\n",
    "\n",
    "        # 각 도메인의 이미지를 입력합니다. Keras 의 Input 을 사용\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # 각각의 서로 다른 도메인으로 이미지를 바꿔(Translate)줌\n",
    "        # 사과, 오렌지 도메인이 있다고 할 때 사과는 오렌지처럼, 오렌지는 사과처럼 바꿔줌\n",
    "        fake_B = self.g_AB(img_A)\n",
    "        fake_A = self.g_BA(img_B)\n",
    "        # 이미지의 원래 도메인으로 다시 바꿔(Translate)줌\n",
    "        reconstr_A = self.g_BA(fake_B)\n",
    "        reconstr_B = self.g_AB(fake_A)\n",
    "        # Identity Loss 를 위해\n",
    "        img_A_id = self.g_BA(img_A)\n",
    "        img_B_id = self.g_AB(img_B)\n",
    "\n",
    "        # generaotr를 학습할 때는 discriminator는 학습하지 않습니다.\n",
    "        self.d_A.trainable = False\n",
    "        self.d_B.trainable = False\n",
    "\n",
    "        valid_A = self.d_A(fake_A)\n",
    "        valid_B = self.d_B(fake_B)\n",
    "\n",
    "        self.combined = Model(inputs=[img_A, img_B],\n",
    "                              outputs=[ valid_A, valid_B,\n",
    "                                        reconstr_A, reconstr_B,\n",
    "                                        img_A_id, img_B_id ])\n",
    "        self.combined.compile(loss=['mse', 'mse',\n",
    "                                    'mae', 'mae',\n",
    "                                    'mae', 'mae'],\n",
    "                            loss_weights=[  1, 1,\n",
    "                                            self.lambda_cycle, self.lambda_cycle,\n",
    "                                            self.lambda_id, self.lambda_id ],\n",
    "                            optimizer=optimizer)\n",
    "        \n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4):\n",
    "            \"\"\"Downsampling 하는 레이어\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Upsampling 하는 레이어\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = InstanceNormalization()(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # 이미지 입력. Keras 의 Input 을 사용.\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d4, d3, self.gf*4)\n",
    "        u2 = deconv2d(u1, d2, self.gf*2)\n",
    "        u3 = deconv2d(u2, d1, self.gf)\n",
    "\n",
    "        u4 = UpSampling2D(size=2)(u3)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "    \n",
    "    def build_discriminator(self): \n",
    " \n",
    " \n",
    "         def d_layer(layer_input, filters, f_size=4, normalization=True): \n",
    "             \"\"\"Discriminator layer\"\"\" \n",
    "             d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input) \n",
    "             d = LeakyReLU(alpha=0.2)(d) \n",
    "             if normalization: \n",
    "                 d = InstanceNormalization()(d) \n",
    "             return d \n",
    " \n",
    " \n",
    "         img = Input(shape=self.img_shape) \n",
    " \n",
    " \n",
    "         d1 = d_layer(img, self.df, normalization=False) \n",
    "         d2 = d_layer(d1, self.df*2) \n",
    "         d3 = d_layer(d2, self.df*4) \n",
    "         d4 = d_layer(d3, self.df*8) \n",
    "\n",
    " \n",
    "         validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4) \n",
    " \n",
    " \n",
    "         return Model(img, validity)\n",
    "    \n",
    "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
    "\n",
    "                # ----------------------\n",
    "                #  Train Discriminators\n",
    "                # ----------------------\n",
    "\n",
    "                # 반대의 도메인으로 이미지 translate\n",
    "                fake_B = self.g_AB.predict(imgs_A)\n",
    "                fake_A = self.g_BA.predict(imgs_B)\n",
    "\n",
    "                # discriminators 학습\n",
    "                # (original images = real / translated = Fake)\n",
    "                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
    "                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
    "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
    "                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
    "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "                # Total disciminator loss\n",
    "                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "\n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "\n",
    "                # generators 학습\n",
    "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n",
    "                                                        [valid, valid,\n",
    "                                                        imgs_A, imgs_B,\n",
    "                                                        imgs_A, imgs_B])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "                # Plot the progress\n",
    "                print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
    "                                                                        % ( epoch, epochs,\n",
    "                                                                            batch_i, self.data_loader.n_batches,\n",
    "                                                                            d_loss[0], 100*d_loss[1],\n",
    "                                                                            g_loss[0],\n",
    "                                                                            np.mean(g_loss[1:3]),\n",
    "                                                                            np.mean(g_loss[3:5]),\n",
    "                                                                            np.mean(g_loss[5:6]),\n",
    "                                                                            elapsed_time))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.sample_images(epoch, batch_i)\n",
    "                    \n",
    "    def sample_images(self, epoch, batch_i): \n",
    "         os.makedirs('images/%s' % self.dataset_name, exist_ok=True) \n",
    "         r, c = 2, 3 \n",
    " \n",
    " \n",
    "         imgs_A = self.data_loader.load_data(domain=\"A\", batch_size=1, is_testing=True) \n",
    "         imgs_B = self.data_loader.load_data(domain=\"B\", batch_size=1, is_testing=True) \n",
    " \n",
    " \n",
    "         # Demo (for GIF) \n",
    "         #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg') \n",
    "         #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg') \n",
    " \n",
    " \n",
    "         # Translate images to the other domain \n",
    "         fake_B = self.g_AB.predict(imgs_A) \n",
    "         fake_A = self.g_BA.predict(imgs_B) \n",
    "         # Translate back to original domain \n",
    "         reconstr_A = self.g_BA.predict(fake_B) \n",
    "         reconstr_B = self.g_AB.predict(fake_A) \n",
    "\n",
    " \n",
    "         gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B]) \n",
    " \n",
    " \n",
    "         # Rescale images 0 - 1 \n",
    "         gen_imgs = 0.5 * gen_imgs + 0.5 \n",
    " \n",
    " \n",
    "         titles = ['Original', 'Translated', 'Reconstructed'] \n",
    "         fig, axs = plt.subplots(r, c) \n",
    "         cnt = 0 \n",
    "         for i in range(r): \n",
    "             for j in range(c): \n",
    "                 axs[i,j].imshow(gen_imgs[cnt]) \n",
    "                 axs[i, j].set_title(titles[j]) \n",
    "                 axs[i,j].axis('off') \n",
    "                 cnt += 1 \n",
    "         fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i)) \n",
    "         plt.close() \n",
    " \n",
    "    if __name__ == '__main__': \n",
    "      gan = CycleGAN() \n",
    "      gan.train(epochs=200, batch_size=1, sample_interval=200) \n",
    "\n",
    " \n",
    "   \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generative",
   "language": "python",
   "name": "generative"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
